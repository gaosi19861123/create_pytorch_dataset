{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dso-s.gao\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\dso-s.gao\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\dso-s.gao\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\dso-s.gao\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\dso-s.gao\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#torch_vision\n",
    "#データ前処理\n",
    "import torch as t\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "path = \"C:/Users/dso-s.gao/Desktop/signate\"\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.\n",
    "---\n",
    "---\n",
    " - The input to the model is expected to be a list of tensors, each of shape [C, H, W], <br>\n",
    "   one for each image, and should be in 0-1 range.   Different images can have different sizes.\n",
    "\n",
    " - The behavior of the model changes depending if it is in training or evaluation mode.\n",
    "\n",
    " - During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:\n",
    "\n",
    " - boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, <br>\n",
    "   with values between 0 and H and 0 and W\n",
    "\n",
    " - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
    "\n",
    " - The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN.\n",
    "\n",
    " - During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], <br>\n",
    "   one for each input image. The fields of the Dict are as follows:\n",
    "\n",
    " - boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W\n",
    "\n",
    " - labels (Int64Tensor[N]): the predicted labels for each image\n",
    "\n",
    " - scores (Tensor[N]): the scores or each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonファイルの中身を解読\n",
    "def read_json_file(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        file = json.load(f)\n",
    "\n",
    "    address = file[\"attributes\"]['route']\n",
    "    time = file[\"attributes\"]['timeofday']\n",
    "\n",
    "    bbox = [row[\"box2d\"] for row in file[\"labels\"]]\n",
    "    category = [row[\"category\"] for row in file[\"labels\"]]\n",
    "    \n",
    "    return address, time, bbox, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カテゴリのラベルを定義します。\n",
    "dict_category = {\n",
    "    'Car': 1,\n",
    "    'Bicycle': 2,\n",
    "    'Pedestrian': 3,\n",
    "    'Signal': 4,\n",
    "    'Signs': 5,\n",
    "    'Truck': 6,\n",
    "    'Bus': 7,\n",
    "    'SVehicle': 8,\n",
    "    'Motorbike': 9,\n",
    "    'Train': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "#統一前処理 \n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, anotation_root, transforms=True, train=True, category_dict=dict_category):\n",
    "        self.root = root\n",
    "        self.anotation_root = anotation_root\n",
    "        self.category_dict = category_dict\n",
    "        self.transforms = transforms\n",
    "        # 下载所有图像文件，为其排序\n",
    "        # 确保它们对齐\n",
    "        self.imgs = list(sorted(os.listdir(root)))\n",
    "        self.anotation = list(sorted(os.listdir(anotation_root)))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #画像データ、PILで読み、TO_TENSORで返す\n",
    "        self.img_path = os.path.join( self.root + self.imgs[index] )\n",
    "        img = Image.open(self.img_path).convert(\"RGB\")\n",
    "        json = self.anotation[index]\n",
    "        \n",
    "        #画像を編集するときであれば\n",
    "        #The input to the model is expected to be a list of tensors, \n",
    "        #each of shape [C, H, W], one for each image, and should be in 0-1 range. \n",
    "        #Different images can have different sizes.\n",
    "        \n",
    "        if self.transforms:\n",
    "            transform = transforms.Compose([\n",
    "                            #transforms.CenterCrop((100, 100)), #中心クロップ\n",
    "                            #transforms.Grayscale(num_output_channels=1), #灰色化\n",
    "                            #transforms.RandomHorizontalFlip(), #水平反転\n",
    "                            #transforms.Scale(224), #resize\n",
    "                            #transforms.TenCrop(3), # 十分割\n",
    "                            #transforms.Lambda(lambda crops : t.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                            #transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], #標準化\n",
    "                            #std = [ 0.229, 0.224, 0.225 ]),\n",
    "                            #transforms.ToPILImage(mode=\"RGBA\") \n",
    "                            transforms.ToTensor() #テンソル化\n",
    "                            ])\n",
    "            img = transform(img)\n",
    "        \n",
    "        #データのBBOXを書き込み\n",
    "        address, time, bbox, category = read_json_file(self.anotation_root + self.anotation[index])\n",
    "        \n",
    "        #boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, \n",
    "        #with values between 0 and H and 0 and W\n",
    "        #boxes の座標をembedingします。\n",
    "        num_objs = len(bbox)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            x1 = bbox[i][\"x1\"]\n",
    "            x2 = bbox[i][\"x2\"]\n",
    "            y1 = bbox[i][\"y1\"]\n",
    "            y2 = bbox[i][\"y2\"]\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "        \n",
    "        #bounding_boxをテンソル化\n",
    "        boxes = t.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        #categoryをテンソル化します。\n",
    "        category_transed = [self.category_dict[cate] for cate in category]\n",
    "        category_transed = t.as_tensor(category_transed, dtype=t.int)\n",
    " \n",
    "        return img, boxes, category_transed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの作成\n",
    "data_set = PennFudanDataset(\n",
    "    root=path + \"/dtc_train_images_0/dtc_train_images/\", \n",
    "    anotation_root=path + \"/dtc_train_annotations/dtc_train_annotations/\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object検知時に自作のデータセット関数を導入\n",
    "def my_collate_fn(batch):\n",
    "    # datasetの出力が\n",
    "    # [image, target] = dataset[batch_idx]\n",
    "    # の場合.\n",
    "    images = []\n",
    "    boxes = []\n",
    "    category = [] \n",
    "    for sample in batch:\n",
    "        image, box, cate = sample\n",
    "        images.append(image)\n",
    "        boxes.append(box)\n",
    "        category.append(cate)\n",
    "        \n",
    "    images = torch.stack(images, dim=0)\n",
    "    return [images, boxes, category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(data_set, shuffle=True, batch_size=2, drop_last=True, num_workers=0, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 869.,  592., 1097.,  779.],\n",
      "        [1067.,  516., 1180.,  649.],\n",
      "        [1173.,  584., 1382.,  725.]]), tensor([[   0.,  537.,   58.,  787.],\n",
      "        [  74.,  540.,  157.,  769.],\n",
      "        [ 132.,  522.,  193.,  699.],\n",
      "        [ 173.,  545.,  216.,  714.],\n",
      "        [ 800.,  497.,  950.,  608.],\n",
      "        [1656.,  581., 1929.,  671.],\n",
      "        [ 412.,  557.,  434.,  600.],\n",
      "        [ 594.,  560.,  627.,  606.],\n",
      "        [ 627.,  562.,  650.,  606.],\n",
      "        [ 651.,  563.,  668.,  606.],\n",
      "        [ 658.,  565.,  679.,  605.],\n",
      "        [ 676.,  567.,  696.,  606.],\n",
      "        [ 712.,  572.,  731.,  611.],\n",
      "        [ 726.,  567.,  745.,  615.],\n",
      "        [ 752.,  567.,  770.,  610.],\n",
      "        [ 764.,  567.,  788.,  615.],\n",
      "        [ 778.,  567.,  801.,  611.],\n",
      "        [ 793.,  567.,  811.,  618.],\n",
      "        [ 821.,  567.,  839.,  603.],\n",
      "        [ 843.,  567.,  858.,  601.],\n",
      "        [ 858.,  583.,  877.,  615.],\n",
      "        [ 810.,  484.,  826.,  519.],\n",
      "        [ 887.,  437.,  907.,  485.],\n",
      "        [ 612.,  408.,  637.,  457.],\n",
      "        [ 955.,  479.,  984.,  500.],\n",
      "        [1088.,  581., 1140.,  620.],\n",
      "        [1148.,  581., 1210.,  625.],\n",
      "        [1209.,  583., 1258.,  626.],\n",
      "        [1247.,  590., 1303.,  639.],\n",
      "        [1324.,  591., 1346.,  638.],\n",
      "        [1326.,  467., 1346.,  510.],\n",
      "        [1478.,  601., 1501.,  649.],\n",
      "        [1509.,  601., 1527.,  653.],\n",
      "        [1535.,  605., 1557.,  654.],\n",
      "        [1548.,  601., 1570.,  648.]])] [tensor([1, 6, 1], dtype=torch.int32), tensor([3, 3, 3, 3, 8, 8, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3,\n",
      "        4, 1, 1, 1, 9, 3, 3, 3, 3, 3, 3], dtype=torch.int32)]\n"
     ]
    }
   ],
   "source": [
    "for step, (img, boxes, category_transed) in enumerate(train_dataloader):\n",
    "    if step <= 2:\n",
    "        print(boxes,category_transed)\n",
    "        break  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
