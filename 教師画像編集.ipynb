{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_vision\n",
    "#データ前処理\n",
    "import torch as t\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils import data\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "path = \"C:/Users/dso-s.gao/Desktop/signate\"\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonファイルの中身を解読\n",
    "def read_json_file(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        file = json.load(f)\n",
    "\n",
    "    address = file[\"attributes\"]['route']\n",
    "    time = file[\"attributes\"]['timeofday']\n",
    "\n",
    "    bbox = [row[\"box2d\"] for row in file[\"labels\"]]\n",
    "    category = [row[\"category\"] for row in file[\"labels\"]]\n",
    "    \n",
    "    return address, time, bbox, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カテゴリのラベルを定義します。\n",
    "dict_category = {\n",
    "    'Car': 1,\n",
    "    'Bicycle': 2,\n",
    "    'Pedestrian': 3,\n",
    "    'Signal': 4,\n",
    "    'Signs': 5,\n",
    "    'Truck': 6,\n",
    "    'Bus': 7,\n",
    "    'SVehicle': 8,\n",
    "    'Motorbike': 9,\n",
    "    'Train': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "#統一前処理 \n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, anotation_root, transforms=True, train=True, category_dict=dict_category):\n",
    "        self.root = root\n",
    "        self.anotation_root = anotation_root\n",
    "        self.category_dict = category_dict\n",
    "        self.transforms = transforms\n",
    "        # 下载所有图像文件，为其排序\n",
    "        # 确保它们对齐\n",
    "        self.imgs = list(sorted(os.listdir(root)))\n",
    "        self.anotation = list(sorted(os.listdir(anotation_root)))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #画像データ、PILで読み、TO_TENSORで返す\n",
    "        self.img_path = os.path.join( self.root + self.imgs[index] )\n",
    "        img = Image.open(self.img_path).convert(\"RGB\")\n",
    "        json = self.anotation[index]\n",
    "        \n",
    "        #画像を編集するときであれば\n",
    "        if self.transforms:\n",
    "            transform = transforms.Compose([\n",
    "                            #transforms.CenterCrop((100, 100)), #中心クロップ\n",
    "                            #transforms.Grayscale(num_output_channels=1), #灰色化\n",
    "                            #transforms.RandomHorizontalFlip(), #水平反転\n",
    "                            #transforms.Scale(224), #resize\n",
    "                            #transforms.TenCrop(3), # 十分割\n",
    "                            #transforms.Lambda(lambda crops : t.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                            #transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], #標準化\n",
    "                            #std = [ 0.229, 0.224, 0.225 ]),\n",
    "                            #transforms.ToPILImage(mode=\"RGBA\") \n",
    "                            transforms.ToTensor() #テンソル化\n",
    "                            ])\n",
    "            img = transform(img)\n",
    "        \n",
    "        #データのBBOXを書き込み\n",
    "        address, time, bbox, category = read_json_file(self.anotation_root + self.anotation[index])\n",
    "        \n",
    "        #boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, \n",
    "        #with values between 0 and H and 0 and W\n",
    "        #boxes の座標をembedingします。\n",
    "        num_objs = len(bbox)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            x1 = bbox[i][\"x1\"]\n",
    "            x2 = bbox[i][\"x2\"]\n",
    "            y1 = bbox[i][\"y1\"]\n",
    "            y2 = bbox[i][\"y2\"]\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "        \n",
    "        #bounding_boxをテンソル化\n",
    "        boxes = t.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        #categoryをテンソル化します。\n",
    "        category_transed = [self.category_dict[cate] for cate in category]\n",
    "        category_transed = t.as_tensor(category_transed, dtype=t.int)\n",
    " \n",
    "        return img, boxes, category_transed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = PennFudanDataset(\n",
    "    root=path + \"/dtc_train_images_0/dtc_train_images/\", \n",
    "    anotation_root=path + \"/dtc_train_annotations/dtc_train_annotations/\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0235, 0.0314, 0.0314,  ..., 0.0314, 0.0235, 0.0196],\n",
       "          [0.0314, 0.0353, 0.0353,  ..., 0.0196, 0.0196, 0.0157],\n",
       "          [0.0314, 0.0353, 0.0392,  ..., 0.0078, 0.0118, 0.0118],\n",
       "          ...,\n",
       "          [0.0196, 0.0196, 0.0196,  ..., 0.0196, 0.0235, 0.0235],\n",
       "          [0.0196, 0.0235, 0.0196,  ..., 0.0235, 0.0235, 0.0157],\n",
       "          [0.0235, 0.0275, 0.0235,  ..., 0.0235, 0.0235, 0.0157]],\n",
       " \n",
       "         [[0.0275, 0.0275, 0.0275,  ..., 0.0314, 0.0235, 0.0196],\n",
       "          [0.0275, 0.0314, 0.0314,  ..., 0.0196, 0.0196, 0.0157],\n",
       "          [0.0314, 0.0353, 0.0392,  ..., 0.0078, 0.0118, 0.0118],\n",
       "          ...,\n",
       "          [0.0000, 0.0039, 0.0039,  ..., 0.0196, 0.0196, 0.0196],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0157, 0.0078],\n",
       "          [0.0000, 0.0039, 0.0039,  ..., 0.0157, 0.0157, 0.0078]],\n",
       " \n",
       "         [[0.0431, 0.0471, 0.0471,  ..., 0.0314, 0.0235, 0.0275],\n",
       "          [0.0471, 0.0510, 0.0510,  ..., 0.0196, 0.0196, 0.0157],\n",
       "          [0.0392, 0.0431, 0.0471,  ..., 0.0000, 0.0039, 0.0118],\n",
       "          ...,\n",
       "          [0.0039, 0.0078, 0.0078,  ..., 0.0196, 0.0118, 0.0118],\n",
       "          [0.0118, 0.0157, 0.0157,  ..., 0.0275, 0.0196, 0.0118],\n",
       "          [0.0157, 0.0196, 0.0196,  ..., 0.0275, 0.0196, 0.0118]]]),\n",
       " tensor([[   0.,  620.,  165.,  691.],\n",
       "         [ 142.,  581.,  211.,  746.],\n",
       "         [ 369.,  555.,  432.,  731.],\n",
       "         [ 772.,  560.,  806.,  620.],\n",
       "         [ 839.,  578.,  881.,  620.],\n",
       "         [ 828.,  553.,  858.,  600.],\n",
       "         [1177.,  553., 1225.,  661.],\n",
       "         [1360.,  555., 1445.,  692.],\n",
       "         [1497.,  578., 1580.,  749.],\n",
       "         [1601.,  586., 1695.,  765.],\n",
       "         [1666.,  580., 1745.,  754.],\n",
       "         [1819.,  563., 1929.,  812.],\n",
       "         [1674.,  586., 1936., 1017.],\n",
       "         [ 889.,  356.,  961.,  389.],\n",
       "         [ 778.,  426.,  815.,  459.],\n",
       "         [1102.,  210., 1177.,  313.],\n",
       "         [ 925.,  558., 1001.,  618.],\n",
       "         [ 965.,  567., 1065.,  646.],\n",
       "         [ 777.,  369.,  811.,  426.]]),\n",
       " tensor([2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 4, 5, 5, 1, 1, 5],\n",
       "        dtype=torch.int32))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-16-7c04d216464a>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-7c04d216464a>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    class PennFudanDataset(object):\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    " \n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # 下载所有图像文件，为其排序\n",
    "        # 确保它们对齐\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"JPGImages\"))))\n",
    "        #self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        #mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # 请注意我们还没有将mask转换为RGB,\n",
    "        # 因为每种颜色对应一个不同的实例\n",
    "        # 0是背景\n",
    "        #mask = Image.open(mask_path)\n",
    "        # 将PIL图像转换为numpy数组\n",
    "        #mask = np.array(mask)\n",
    "        # 实例被编码为不同的颜色\n",
    "        #obj_ids = np.unique(mask)\n",
    "        # 第一个id是背景，所以删除它\n",
    "        #obj_ids = obj_ids[1:]\n",
    " \n",
    "        # 将颜色编码的mask分成一组\n",
    "        # 二进制格式\n",
    "        #masks = mask == obj_ids[:, None, None]\n",
    " \n",
    "        # 获取每个mask的边界框坐标\n",
    "        #num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    " \n",
    "        # 将所有转换为torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # 这里仅有一个类\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        #masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    " \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # 假设所有实例都不是人群\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    " \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    " \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    " \n",
    "        return img, target\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像ファイルパスから読み込み\n",
    "def read(filename): \n",
    "    return Image.open(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.\n",
    "\n",
    "The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes.\n",
    "\n",
    "The behavior of the model changes depending if it is in training or evaluation mode.\n",
    "\n",
    "During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:\n",
    "\n",
    "boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W\n",
    "\n",
    "labels (Int64Tensor[N]): the class label for each ground-truth box\n",
    "\n",
    "The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN.\n",
    "\n",
    "During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows:\n",
    "\n",
    "boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W\n",
    "\n",
    "labels (Int64Tensor[N]): the predicted labels for each image\n",
    "\n",
    "scores (Tensor[N]): the scores or each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./VOC_Detection\\VOCtrainval_11-May-2012.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999642624it [10:20, 4304268.42it/s]                                                                                   "
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "data = datasets.VOCDetection('./VOC_Detection',year='2012', image_set='train', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotation': {'folder': 'VOC2012',\n",
       "  'filename': '2008_000008.jpg',\n",
       "  'source': {'database': 'The VOC2008 Database',\n",
       "   'annotation': 'PASCAL VOC2008',\n",
       "   'image': 'flickr'},\n",
       "  'size': {'width': '500', 'height': '442', 'depth': '3'},\n",
       "  'segmented': '0',\n",
       "  'object': [{'name': 'horse',\n",
       "    'pose': 'Left',\n",
       "    'truncated': '0',\n",
       "    'occluded': '1',\n",
       "    'bndbox': {'xmin': '53', 'ymin': '87', 'xmax': '471', 'ymax': '420'},\n",
       "    'difficult': '0'},\n",
       "   {'name': 'person',\n",
       "    'pose': 'Unspecified',\n",
       "    'truncated': '1',\n",
       "    'occluded': '0',\n",
       "    'bndbox': {'xmin': '158', 'ymin': '44', 'xmax': '289', 'ymax': '167'},\n",
       "    'difficult': '0'}]}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
